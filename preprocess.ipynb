{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pykospacing import Spacing\n",
    "from konlpy.tag import Okt \n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "tokenized_data = []\n",
    "tokenized_data_valid = []\n",
    "file_list = []\n",
    "y_train = []\n",
    "y_valid = []\n",
    "max_len = 500\n",
    "file_len = 0\n",
    "file_len_valid = 0\n",
    "embedding_dim = 100\n",
    "category = [\"농림.축산지원\", \"도시개발\", \"산업진흥\", \"상.하수도관리\", \"인.허가\", \"일반행정\", \"주민복지\", \"주민생활지원\", \"주민자치\", \"지역문화\", \"지연환경.산림\", \"회계.예산\"]\n",
    "train_paths = [\"./Training/train/01.라벨링데이터(Json)\", \"./Training/train2\"]\n",
    "valid_paths = [\"./Validation/valid/01.라벨링데이터(Json)\"]\n",
    "filePath_x = './X_train.txt'\n",
    "filePath_y = './Y_train.txt'\n",
    "filePath_valx = './X_valid.txt'\n",
    "filePath_valy = './Y_valid.txt'\n",
    "\n",
    "hangul = re.compile('[^ ㄱ-ㅣ가-힣+]')\n",
    "\n",
    "def get_tokenized(file_path):\n",
    "    label = []\n",
    "    with open(file_path, 'r', encoding='UTF8') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    org_sentence = \"\"\n",
    "    \n",
    "    for i in range(len(json_data['annotations'])):\n",
    "        temp = str(json_data['annotations'][i]['annotation.text'])\n",
    "        if temp == ' ':\n",
    "            continue\n",
    "        org_sentence += temp\n",
    "    lab = str(json_data['images'][0]['image.category'])\n",
    "    if lab in category:\n",
    "        label.append(str(json_data['images'][0]['image.category']))\n",
    "    else:\n",
    "        if lab == '농립.축산지원':\n",
    "            label.append('농림.축산지원')\n",
    "        elif lab == '상수도관리':\n",
    "            label.append('상.하수도관리')\n",
    "        \n",
    "    spacing = Spacing()\n",
    "    okt = Okt()\n",
    "    prep_sentence = hangul.sub('', org_sentence)\n",
    "    space_sentence = spacing(prep_sentence)\n",
    "    tok_sentence = okt.nouns(space_sentence)\n",
    "    return tok_sentence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ver = 0\n",
    "#finished = ['농림.축산지원', '도시개발', '산업진흥', '상.하수도관리', '인.허가', '일반행정', '주민복지', '주민생활지원']\n",
    "file_max = 100\n",
    "for dir_path in train_paths:\n",
    "    #ver += 1\n",
    "    for cate in category:\n",
    "        file_len = 0\n",
    "        cate_path = os.path.join(dir_path, cate)\n",
    "        for (root, directories, files) in os.walk(cate_path):            \n",
    "            print(\"Saving directory : \", root)\n",
    "            if file_len == file_max:\n",
    "                break\n",
    "            for file in files:\n",
    "                if file_len == file_max:\n",
    "                    break\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    tok_sentence, label = get_tokenized(file_path)\n",
    "                    tokenized_data.append(tok_sentence)\n",
    "                    #tmp.append(tok_sentence)\n",
    "                    y_train.append(label)\n",
    "                    #tmp_y.append(label)\n",
    "                    file_len += 1\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "with open('tokenized_data.txt', 'wb') as lf:\n",
    "    pickle.dump(tokenized_data, lf)\n",
    "with open('y_train_org.txt', 'wb') as lf:\n",
    "    pickle.dump(y_train, lf)\n",
    "    \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_data)\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "tokenizer = Tokenizer(vocab_size)\n",
    "tokenizer.fit_on_texts(tokenized_data)\n",
    "X_train = tokenizer.texts_to_sequences(tokenized_data)\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "\n",
    "tokenizer_y = Tokenizer()\n",
    "tokenizer_y.fit_on_texts(y_train)\n",
    "Y_train = tokenizer_y.texts_to_sequences(y_train)\n",
    "Y_train = to_categorical(Y_train)\n",
    "#print(Y_train.shape) # (100, 2)\n",
    "#print(X_train.shape) # (100, 260)\n",
    "\n",
    "# Save X_train, Y_train\n",
    "with open(filePath_x, 'wb') as lf:\n",
    "    pickle.dump(X_train, lf)\n",
    "\n",
    "with open(filePath_y, 'wb') as lf:\n",
    "    pickle.dump(Y_train, lf)\n",
    "    \n",
    "  \n",
    "tokenized_data_valid = []\n",
    "y_valid = []\n",
    "for dir_path in valid_paths:\n",
    "    for cate in category:\n",
    "    #    tmp = []\n",
    "    #    tmp_y = []\n",
    "        file_len_valid = 0\n",
    "        dir_path2 = os.path.join(dir_path, cate)\n",
    "        for (root, directories, files) in os.walk(dir_path2):  \n",
    "            if file_len_valid == file_max:\n",
    "                break          \n",
    "            print(\"Saving directory : \", root)            \n",
    "            for file in files:\n",
    "                if file_len_valid == file_max:\n",
    "                    break  \n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    tok_sentence, label = get_tokenized(file_path)\n",
    "                    tokenized_data_valid.append(tok_sentence)\n",
    "                    y_valid.append(label)\n",
    "                    file_len_valid += 1\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "X_valid = tokenizer.texts_to_sequences(tokenized_data_valid)\n",
    "X_valid = pad_sequences(X_valid, maxlen=max_len)\n",
    "\n",
    "Y_valid = tokenizer_y.texts_to_sequences(y_valid)\n",
    "Y_valid = to_categorical(Y_valid)\n",
    "\n",
    "with open('tokenized_data_valid.txt', 'wb') as lf:\n",
    "    pickle.dump(tokenized_data_valid, lf)\n",
    "    \n",
    "with open('y_valid_org.txt', 'wb') as lf:\n",
    "    pickle.dump(y_valid, lf)\n",
    "\n",
    "# Save X_valid, Y_valid\n",
    "with open(filePath_valx, 'wb') as lf:\n",
    "    pickle.dump(X_valid, lf)\n",
    "\n",
    "with open(filePath_valy, 'wb') as lf:\n",
    "    pickle.dump(Y_valid, lf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be804b82f4b6c4507defaecc1db297e0af07cd9427439add2fdfb9df30e508f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
