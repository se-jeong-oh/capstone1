{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "815/815 [==============================] - 4s 5ms/step - loss: 1.5844 - accuracy: 0.4957 - val_loss: 1.1976 - val_accuracy: 0.6325\n",
      "Epoch 2/100\n",
      "815/815 [==============================] - 4s 5ms/step - loss: 1.1979 - accuracy: 0.6128 - val_loss: 1.1053 - val_accuracy: 0.6528\n",
      "Epoch 3/100\n",
      "815/815 [==============================] - 4s 5ms/step - loss: 1.0478 - accuracy: 0.6594 - val_loss: 1.0362 - val_accuracy: 0.6895\n",
      "Epoch 4/100\n",
      "815/815 [==============================] - 4s 5ms/step - loss: 0.9477 - accuracy: 0.6892 - val_loss: 1.0097 - val_accuracy: 0.6983\n",
      "Epoch 5/100\n",
      "815/815 [==============================] - 4s 5ms/step - loss: 0.8675 - accuracy: 0.7138 - val_loss: 1.0267 - val_accuracy: 0.6971\n",
      "Epoch 6/100\n",
      "815/815 [==============================] - 4s 5ms/step - loss: 0.8002 - accuracy: 0.7343 - val_loss: 1.0427 - val_accuracy: 0.7062\n",
      "Epoch 7/100\n",
      "815/815 [==============================] - 4s 5ms/step - loss: 0.7420 - accuracy: 0.7521 - val_loss: 1.0363 - val_accuracy: 0.7149\n",
      "Epoch 8/100\n",
      "815/815 [==============================] - 4s 5ms/step - loss: 0.6971 - accuracy: 0.7630 - val_loss: 1.0390 - val_accuracy: 0.7233\n",
      "Epoch 9/100\n",
      "815/815 [==============================] - 4s 5ms/step - loss: 0.6427 - accuracy: 0.7855 - val_loss: 1.0452 - val_accuracy: 0.7314\n",
      "Epoch 10/100\n",
      "815/815 [==============================] - 4s 5ms/step - loss: 0.6082 - accuracy: 0.7934 - val_loss: 1.0982 - val_accuracy: 0.7252\n",
      "Epoch 11/100\n",
      "815/815 [==============================] - 4s 5ms/step - loss: 0.5616 - accuracy: 0.8064 - val_loss: 1.1575 - val_accuracy: 0.7218\n",
      "Epoch 12/100\n",
      "815/815 [==============================] - 4s 5ms/step - loss: 0.5286 - accuracy: 0.8176 - val_loss: 1.1652 - val_accuracy: 0.7358\n",
      "345/345 - 0s - loss: 1.7984 - accuracy: 0.5886 - 401ms/epoch - 1ms/step\n",
      "1.7983810901641846 0.5886467099189758\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.legacy import SGD\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "PATH = './data/'\n",
    "train_load_df = pd.read_pickle(PATH + \"train_df.pkl\")\n",
    "\n",
    "N = 6000\n",
    "label_count = train_load_df.label.value_counts()\n",
    "under_list = label_count[label_count > N].index\n",
    "df = pd.DataFrame()\n",
    "for i in label_count.index:\n",
    "    if i in under_list:\n",
    "        df = pd.concat([df, train_load_df[train_load_df.label == i].sample(n=N)])\n",
    "    else:\n",
    "        df = pd.concat([df, train_load_df[train_load_df.label == i]])\n",
    "\n",
    "vocab_size = 10000\n",
    "num_classes = 12\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "\n",
    "X,Y = df.sentence, df.label\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.2, random_state=2, shuffle=True)\n",
    "x_valid, x_test, y_valid, y_test = train_test_split(x_valid, y_valid, test_size=0.2, random_state=2, shuffle=True)\n",
    "\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "X_train = tokenizer.texts_to_matrix(x_train, mode='tfidf')\n",
    "X_valid = tokenizer.texts_to_matrix(x_valid, mode='tfidf')\n",
    "X_test = tokenizer.texts_to_matrix(x_test, mode='tfidf')\n",
    "\n",
    "Y_train = to_categorical(y_train, num_classes)\n",
    "Y_valid = to_categorical(y_valid, num_classes)\n",
    "Y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "smt = SMOTE()\n",
    "X_train_smt, Y_train_smt = smt.fit_resample(X_train, Y_train)\n",
    "BATCH_SIZE = 64\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)\n",
    "sgd = SGD(learning_rate=.01, momentum=.9, nesterov=True)\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=256, activation='elu', input_shape=[input_shape]),\n",
    "    tf.keras.layers.Dense(units=128, activation='elu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=32, activation='swish'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax'),         \n",
    "])\n",
    "#model = load_model('./h5/model_SMOTE.h5')\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_smt, Y_train_smt, batch_size=BATCH_SIZE, epochs=100, verbose=1, validation_split=0.1, callbacks=[early_stop])\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_valid, Y_valid, verbose=2)\n",
    "print(test_loss, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be804b82f4b6c4507defaecc1db297e0af07cd9427439add2fdfb9df30e508f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
